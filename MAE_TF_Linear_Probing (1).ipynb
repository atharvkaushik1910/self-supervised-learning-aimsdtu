{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b31f438f",
      "metadata": {
        "id": "b31f438f"
      },
      "source": [
        "Linear Probing with Pretrained MAE Encoder - TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00844d66",
      "metadata": {
        "id": "00844d66"
      },
      "outputs": [],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c019936d",
      "metadata": {
        "id": "c019936d"
      },
      "source": [
        " Load Labeled Dataset for Linear Probing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7318b7cc",
      "metadata": {
        "id": "7318b7cc"
      },
      "outputs": [],
      "source": [
        "IMAGE_SIZE= 224\n",
        "BATCH_SIZE= 32\n",
        "NUM_CLASSES= 100\n",
        "import json\n",
        "\n",
        "# Load class label mapping\n",
        "with open(\"ssl_dataset/Labels.json\", \"r\") as f:\n",
        "    label_map = json.load(f)\n",
        "class_names = sorted(label_map, key=lambda k: label_map[k])\n",
        "\n",
        "train_dirs = [\"ssl_dataset/train.X1\", \"ssl_dataset/train.X2\", \"ssl_dataset/train.X3\", \"ssl_dataset/train.X4\"]\n",
        "val_dir = \"ssl_dataset/val.X\"\n",
        "\n",
        "# Load and normalize training dataset\n",
        "all_train_ds = []\n",
        "for path in train_dirs:\n",
        "    ds = image_dataset_from_directory(\n",
        "        path,\n",
        "        labels='inferred',\n",
        "        label_mode='int',\n",
        "        class_names=class_names,\n",
        "        image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True\n",
        "    )\n",
        "    ds = ds.map(lambda x, y: (x / 255.0, y))\n",
        "    all_train_ds.append(ds)\n",
        "\n",
        "train_dataset = all_train_ds[0]\n",
        "for ds in all_train_ds[1:]:\n",
        "    train_dataset = train_dataset.concatenate(ds)\n",
        "\n",
        "# Load and normalize validation dataset\n",
        "val_dataset = image_dataset_from_directory(\n",
        "    val_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='int',\n",
        "    class_names=class_names,\n",
        "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False\n",
        ")\n",
        "val_dataset = val_dataset.map(lambda x, y: (x / 255.0, y))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17d96d93",
      "metadata": {
        "id": "17d96d93"
      },
      "source": [
        "Patchify Function for Embedding Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1aff9e0",
      "metadata": {
        "id": "a1aff9e0"
      },
      "outputs": [],
      "source": [
        "\n",
        "def patchify(images, patch_size=16):\n",
        "    batch_size = tf.shape(images)[0]\n",
        "    patches = tf.image.extract_patches(\n",
        "        images=images,\n",
        "        sizes=[1, patch_size, patch_size, 1],\n",
        "        strides=[1, patch_size, patch_size, 1],\n",
        "        rates=[1, 1, 1, 1],\n",
        "        padding='VALID'\n",
        "    )\n",
        "    patch_dims = patches.shape[-1]\n",
        "    patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "    return patches\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f34a3572",
      "metadata": {
        "id": "f34a3572"
      },
      "source": [
        "Load Pretrained Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18269089",
      "metadata": {
        "id": "18269089"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_encoder(input_shape, num_patches, embed_dim):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    x = layers.Dense(embed_dim)(inputs)\n",
        "    x = layers.LayerNormalization()(x)\n",
        "    for _ in range(4):\n",
        "        x1 = layers.LayerNormalization()(x)\n",
        "        x1 = layers.MultiHeadAttention(num_heads=4, key_dim=embed_dim)(x1, x1)\n",
        "        x = layers.Add()([x, x1])\n",
        "    outputs = layers.LayerNormalization()(x)\n",
        "    return models.Model(inputs, outputs, name=\"encoder\")\n",
        "\n",
        "PATCH_SIZE = 16\n",
        "NUM_PATCHES = (IMAGE_SIZE // PATCH_SIZE) ** 2\n",
        "PATCH_DIM = PATCH_SIZE * PATCH_SIZE * 3\n",
        "EMBED_DIM = 128\n",
        "\n",
        "encoder = create_encoder((NUM_PATCHES, PATCH_DIM), NUM_PATCHES, EMBED_DIM)\n",
        "encoder.load_weights(\"mae_encoder_tf.h5\")\n",
        "encoder.trainable = False  # Freeze the encoder\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cbe3792",
      "metadata": {
        "id": "7cbe3792"
      },
      "source": [
        "'Linear Classifier on Top of Frozen Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a7f5274",
      "metadata": {
        "id": "8a7f5274"
      },
      "outputs": [],
      "source": [
        "\n",
        "inputs = layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
        "patches = patchify(inputs)\n",
        "features = encoder(patches)\n",
        "features = layers.GlobalAveragePooling1D()(features)\n",
        "outputs = layers.Dense(NUM_CLASSES, activation='softmax')(features)\n",
        "\n",
        "model = models.Model(inputs, outputs)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b93e224c",
      "metadata": {
        "id": "b93e224c"
      },
      "source": [
        " Train Linear Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f8e1f52",
      "metadata": {
        "id": "9f8e1f52"
      },
      "outputs": [],
      "source": [
        "\n",
        "history = model.fit(train_dataset, validation_data=val_dataset, epochs=3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7698f836",
      "metadata": {
        "id": "7698f836"
      },
      "source": [
        " Evaluation: Accuracy and F1 Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3308e385",
      "metadata": {
        "id": "3308e385"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Get true and predicted labels\n",
        "y_true, y_pred = [], []\n",
        "for x_batch, y_batch in val_dataset:\n",
        "    preds = model.predict(x_batch)\n",
        "    y_true.extend(y_batch.numpy())\n",
        "    y_pred.extend(np.argmax(preds, axis=1))\n",
        "\n",
        "print(classification_report(y_true, y_pred, digits=4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9rN4lOZhJnaB",
      "metadata": {
        "id": "9rN4lOZhJnaB"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('MAE Linear Probing Loss')\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
