{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gCg632gSdrL"
      },
      "source": [
        "Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLANefmgSf59"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAb7F-n7SjW6"
      },
      "source": [
        "Dataset Loading (Unlabeled for Pretraining)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOy99cRSSmaN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "IMAGE_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_dirs = [f\"ssl_dataset/train.X{i}\" for i in range(1, 5)]\n",
        "\n",
        "def load_mae_dataset():\n",
        "    all_datasets = []\n",
        "    for path in train_dirs:\n",
        "        ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "            path,\n",
        "            labels=None,\n",
        "            label_mode=None,\n",
        "            image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "            batch_size=BATCH_SIZE,\n",
        "            shuffle=True\n",
        "        )\n",
        "        ds = ds.map(lambda x: x / 255.0)  # Normalize\n",
        "        all_datasets.append(ds)\n",
        "\n",
        "    # Combine into one\n",
        "    train_ds = all_datasets[0]\n",
        "    for ds in all_datasets[1:]:\n",
        "        train_ds = train_ds.concatenate(ds)\n",
        "\n",
        "    return train_ds\n",
        "\n",
        "train_dataset = load_mae_dataset()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NepcOM-StL7"
      },
      "source": [
        "Patch + Masking Utility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLx9-SdNSuEP"
      },
      "outputs": [],
      "source": [
        "def patchify(images, patch_size=16):\n",
        "    batch_size = tf.shape(images)[0]\n",
        "    patches = tf.image.extract_patches(\n",
        "        images=images,\n",
        "        sizes=[1, patch_size, patch_size, 1],\n",
        "        strides=[1, patch_size, patch_size, 1],\n",
        "        rates=[1, 1, 1, 1],\n",
        "        padding='VALID'\n",
        "    )\n",
        "    patch_dims = patches.shape[-1]\n",
        "    patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "    return patches\n",
        "\n",
        "def random_masking(patches, mask_ratio=0.75):\n",
        "    batch, num_patches, _ = patches.shape\n",
        "    num_mask = int(mask_ratio * num_patches)\n",
        "\n",
        "    mask_indices = np.array([\n",
        "        np.random.choice(num_patches, num_mask, replace=False) for _ in range(batch)\n",
        "    ])\n",
        "\n",
        "    mask = np.ones((batch, num_patches), dtype=np.float32)\n",
        "    for i in range(batch):\n",
        "        mask[i, mask_indices[i]] = 0  # 0 = keep, 1 = mask\n",
        "\n",
        "    return tf.convert_to_tensor(mask), mask_indices\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnAycDr5SzJa"
      },
      "source": [
        "MAE Encoder + Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E25j7aNmS2Dg"
      },
      "outputs": [],
      "source": [
        "def create_encoder(input_shape, num_patches, embed_dim):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    x = layers.Dense(embed_dim)(inputs)\n",
        "    x = layers.LayerNormalization()(x)\n",
        "    for _ in range(4):  # Use more layers for deeper encoder\n",
        "        x1 = layers.LayerNormalization()(x)\n",
        "        x1 = layers.MultiHeadAttention(num_heads=4, key_dim=embed_dim)(x1, x1)\n",
        "        x = layers.Add()([x, x1])\n",
        "    outputs = layers.LayerNormalization()(x)\n",
        "    return models.Model(inputs, outputs, name=\"encoder\")\n",
        "\n",
        "def create_decoder(embed_dim, patch_dim):\n",
        "    inputs = layers.Input(shape=(None, embed_dim))\n",
        "    x = layers.Dense(embed_dim)(inputs)\n",
        "    x = layers.LayerNormalization()(x)\n",
        "    x = layers.Dense(patch_dim)(x)\n",
        "    return models.Model(inputs, x, name=\"decoder\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yM0tFLQKS5BO"
      },
      "source": [
        "MAE Model Wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gy9YcyqAS7Su"
      },
      "outputs": [],
      "source": [
        "class MAE(tf.keras.Model):\n",
        "    def __init__(self, encoder, decoder, num_patches, patch_dim):\n",
        "        super(MAE, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.num_patches = num_patches\n",
        "        self.patch_dim = patch_dim\n",
        "\n",
        "    def call(self, images):\n",
        "        patches = patchify(images)  # [B, N, D]\n",
        "        mask, _ = random_masking(patches)\n",
        "        visible_patches = patches * tf.expand_dims(1 - mask, -1)\n",
        "        latent = self.encoder(visible_patches)\n",
        "        reconstructed = self.decoder(latent)\n",
        "        return reconstructed, patches, mask\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZDoWHBOS-Li"
      },
      "source": [
        "Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JWtTAXuTBLL"
      },
      "outputs": [],
      "source": [
        "PATCH_SIZE = 16\n",
        "NUM_PATCHES = (IMAGE_SIZE // PATCH_SIZE) ** 2\n",
        "PATCH_DIM = PATCH_SIZE * PATCH_SIZE * 3\n",
        "EMBED_DIM = 128\n",
        "\n",
        "encoder = create_encoder((NUM_PATCHES, PATCH_DIM), NUM_PATCHES, EMBED_DIM)\n",
        "decoder = create_decoder(EMBED_DIM, PATCH_DIM)\n",
        "mae = MAE(encoder, decoder, NUM_PATCHES, PATCH_DIM)\n",
        "\n",
        "loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "@tf.function\n",
        "def train_step(images):\n",
        "    with tf.GradientTape() as tape:\n",
        "        reconstructed, original, mask = mae(images)\n",
        "        loss = loss_fn(original * tf.expand_dims(mask, -1), reconstructed * tf.expand_dims(mask, -1))\n",
        "    gradients = tape.gradient(loss, mae.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, mae.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "EPOCHS = 3\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "    for batch in train_dataset:\n",
        "        loss = train_step(batch)\n",
        "    print(f\"Loss: {loss.numpy():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-U9QnAnTD16"
      },
      "source": [
        "Save Encoder for Linear Probing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gd-bojFUA7Vc"
      },
      "outputs": [],
      "source": [
        "encoder.save_weights(\"mae_encoder_tf.h5\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
